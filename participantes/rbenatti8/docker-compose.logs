
 pod4 Pulling 
 pod1 Pulling 
 pod2 Pulling 
 pod3 Pulling 
 507960b01334 Pulling fs layer 
 9abf53cce197 Pulling fs layer 
 507960b01334 Pulling fs layer 
 9abf53cce197 Pulling fs layer 
 507960b01334 Pulling fs layer 
 9abf53cce197 Pulling fs layer 
 9abf53cce197 Downloading [>                                                  ]     82kB/7.824MB
 9abf53cce197 Downloading [>                                                  ]     82kB/7.824MB
 9abf53cce197 Downloading [>                                                  ]     82kB/7.824MB
 507960b01334 Download complete 
 507960b01334 Downloading [==================================================>]      93B/93B
 507960b01334 Download complete 
 507960b01334 Downloading [==================================================>]      93B/93B
 507960b01334 Download complete 
 507960b01334 Extracting [==================================================>]      93B/93B
 507960b01334 Extracting [==================================================>]      93B/93B
 507960b01334 Extracting [==================================================>]      93B/93B
 9abf53cce197 Downloading [============================================>      ]  7.033MB/7.824MB
 9abf53cce197 Downloading [============================================>      ]  7.033MB/7.824MB
 9abf53cce197 Downloading [============================================>      ]  7.033MB/7.824MB
 507960b01334 Pull complete 
 507960b01334 Pull complete 
 507960b01334 Pull complete 
 9abf53cce197 Verifying Checksum 
 9abf53cce197 Verifying Checksum 
 9abf53cce197 Download complete 
 9abf53cce197 Download complete 
 9abf53cce197 Download complete 
 9abf53cce197 Extracting [>                                                  ]   98.3kB/7.824MB
 9abf53cce197 Extracting [>                                                  ]   98.3kB/7.824MB
 9abf53cce197 Extracting [>                                                  ]   98.3kB/7.824MB
 507960b01334 Already exists 
 9abf53cce197 Pulling fs layer 
 9abf53cce197 Extracting [>                                                  ]   98.3kB/7.824MB
 9abf53cce197 Extracting [==================================================>]  7.824MB/7.824MB
 9abf53cce197 Extracting [==================================================>]  7.824MB/7.824MB
 9abf53cce197 Extracting [==================================================>]  7.824MB/7.824MB
 9abf53cce197 Extracting [==================================================>]  7.824MB/7.824MB
 9abf53cce197 Pull complete 
 9abf53cce197 Pull complete 
 9abf53cce197 Pull complete 
 9abf53cce197 Pull complete 
 pod3 Pulled 
 pod1 Pulled 
 pod4 Pulled 
 pod2 Pulled 
 Network rbenatti8_backend  Creating
 Network rbenatti8_backend  Created
 Container redis-cache-rinha  Creating
 Container redis-cache-rinha  Created
 Container pod4  Creating
 Container pod3  Creating
 Container pod2  Creating
 Container pod1  Creating
 Container pod2  Created
 Container pod1  Created
 Container pod3  Created
 Container rbenatti8-haproxy-1  Creating
 Container pod4  Created
 Container rbenatti8-haproxy-1  Created
Attaching to pod1, pod2, pod3, pod4, haproxy-1, redis-cache-rinha
redis-cache-rinha  | 1:C 30 Jul 2025 04:02:31.806 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
redis-cache-rinha  | 1:C 30 Jul 2025 04:02:31.806 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-cache-rinha  | 1:C 30 Jul 2025 04:02:31.806 * Redis version=7.4.5, bits=64, commit=00000000, modified=0, pid=1, just started
redis-cache-rinha  | 1:C 30 Jul 2025 04:02:31.806 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-cache-rinha  | 1:M 30 Jul 2025 04:02:31.806 * monotonic clock: POSIX clock_gettime
redis-cache-rinha  | 1:M 30 Jul 2025 04:02:31.807 * Running mode=standalone, port=6379.
redis-cache-rinha  | 1:M 30 Jul 2025 04:02:31.807 * Server initialized
redis-cache-rinha  | 1:M 30 Jul 2025 04:02:31.807 * Ready to accept connections tcp
pod2               | 2025/07/30 04:02:32 maxprocs: Updating GOMAXPROCS=1: using minimum allowed GOMAXPROCS
pod2               | 2025/07/30 04:02:32 INFO GOMEMLIMIT is updated package=github.com/KimMachineGun/automemlimit/memlimit GOMEMLIMIT=61341696 previous=9223372036854775807
pod3               | 2025/07/30 04:02:32 maxprocs: Updating GOMAXPROCS=1: using minimum allowed GOMAXPROCS
pod3               | 2025/07/30 04:02:32 INFO GOMEMLIMIT is updated package=github.com/KimMachineGun/automemlimit/memlimit GOMEMLIMIT=61341696 previous=9223372036854775807
pod4               | 2025/07/30 04:02:32 maxprocs: Updating GOMAXPROCS=1: using minimum allowed GOMAXPROCS
pod4               | 2025/07/30 04:02:32 INFO GOMEMLIMIT is updated package=github.com/KimMachineGun/automemlimit/memlimit GOMEMLIMIT=61341696 previous=9223372036854775807
pod1               | 2025/07/30 04:02:32 maxprocs: Updating GOMAXPROCS=1: using minimum allowed GOMAXPROCS
pod1               | 2025/07/30 04:02:32 INFO GOMEMLIMIT is updated package=github.com/KimMachineGun/automemlimit/memlimit GOMEMLIMIT=61341696 previous=9223372036854775807
haproxy-1          | [NOTICE]   (1) : Initializing new worker (8)
haproxy-1          | [NOTICE]   (1) : Loading success.
pod1               | panic: runtime error: invalid memory address or nil pointer dereference
pod1               | [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x7e0985]
pod1               | 
pod1               | goroutine 1137 [running]:
pod1               | github.com/rbenatti8/rinha-de-backend-2025/internal/server.(*Handler).handleGetSummary(0xc000408a80, 0xc000272008)
pod1               | 	/app/internal/server/fasthttp.go:95 +0x45
pod1               | github.com/rbenatti8/rinha-de-backend-2025/internal/server.(*Handler).handleGet(0xc000408a80, 0xc000272008)
pod1               | 	/app/internal/server/fasthttp.go:84 +0x85
pod1               | github.com/rbenatti8/rinha-de-backend-2025/internal/server.(*Handler).handler(0xc00017acb8?, 0x49b0b3?)
pod1               | 	/app/internal/server/fasthttp.go:33 +0x48
pod1               | github.com/valyala/fasthttp.(*Server).serveConn(0xc0000c4248, {0xa0e6f8, 0xc00004a160})
pod1               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/server.go:2455 +0x11cf
pod1               | github.com/valyala/fasthttp.(*workerPool).workerFunc(0xc0000c2090, 0xc0002296a0)
pod1               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/workerpool.go:225 +0x92
pod1               | github.com/valyala/fasthttp.(*workerPool).getCh.func1()
pod1               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/workerpool.go:197 +0x32
pod1               | created by github.com/valyala/fasthttp.(*workerPool).getCh in goroutine 794
pod1               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/workerpool.go:196 +0x194
[Kpod1 exited with code 2
pod2               | panic: runtime error: invalid memory address or nil pointer dereference
pod2               | [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x7e0985]
pod2               | 
pod2               | goroutine 1667 [running]:
pod2               | github.com/rbenatti8/rinha-de-backend-2025/internal/server.(*Handler).handleGetSummary(0xc00025ad68, 0xc0000e2008)
pod2               | 	/app/internal/server/fasthttp.go:95 +0x45
pod2               | github.com/rbenatti8/rinha-de-backend-2025/internal/server.(*Handler).handleGet(0xc00025ad68, 0xc0000e2008)
pod2               | 	/app/internal/server/fasthttp.go:84 +0x85
pod2               | github.com/rbenatti8/rinha-de-backend-2025/internal/server.(*Handler).handler(0xc0004a7cb8?, 0x49b0b3?)
pod2               | 	/app/internal/server/fasthttp.go:33 +0x48
pod2               | github.com/valyala/fasthttp.(*Server).serveConn(0xc000172248, {0xa0e6f8, 0xc00004a070})
pod2               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/server.go:2455 +0x11cf
pod2               | github.com/valyala/fasthttp.(*workerPool).workerFunc(0xc000170090, 0xc0002e97a0)
pod2               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/workerpool.go:225 +0x92
pod2               | github.com/valyala/fasthttp.(*workerPool).getCh.func1()
pod2               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/workerpool.go:197 +0x32
pod2               | created by github.com/valyala/fasthttp.(*workerPool).getCh in goroutine 803
pod2               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/workerpool.go:196 +0x194
[Kpod2 exited with code 2
pod3               | panic: runtime error: invalid memory address or nil pointer dereference
pod3               | [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x7e0985]
pod3               | 
pod3               | goroutine 2164 [running]:
pod3               | github.com/rbenatti8/rinha-de-backend-2025/internal/server.(*Handler).handleGetSummary(0xc00017b350, 0xc00030c008)
pod3               | 	/app/internal/server/fasthttp.go:95 +0x45
pod3               | github.com/rbenatti8/rinha-de-backend-2025/internal/server.(*Handler).handleGet(0xc00017b350, 0xc00030c008)
pod3               | 	/app/internal/server/fasthttp.go:84 +0x85
pod3               | github.com/rbenatti8/rinha-de-backend-2025/internal/server.(*Handler).handler(0xc000199cb8?, 0x49b0b3?)
pod3               | 	/app/internal/server/fasthttp.go:33 +0x48
pod3               | github.com/valyala/fasthttp.(*Server).serveConn(0xc0000f2248, {0xa0e6f8, 0xc00004a068})
pod3               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/server.go:2455 +0x11cf
pod3               | github.com/valyala/fasthttp.(*workerPool).workerFunc(0xc0000f0090, 0xc0002c97a0)
pod3               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/workerpool.go:225 +0x92
pod3               | github.com/valyala/fasthttp.(*workerPool).getCh.func1()
pod3               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/workerpool.go:197 +0x32
pod3               | created by github.com/valyala/fasthttp.(*workerPool).getCh in goroutine 807
pod3               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/workerpool.go:196 +0x194
[Kpod3 exited with code 2
haproxy-1          | [WARNING]  (8) : Server http_back/web1 is DOWN, reason: Layer4 timeout, check duration: 2002ms. 3 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.
pod4               | panic: runtime error: invalid memory address or nil pointer dereference
pod4               | [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x7e0985]
pod4               | 
pod4               | goroutine 2647 [running]:
pod4               | github.com/rbenatti8/rinha-de-backend-2025/internal/server.(*Handler).handleGetSummary(0xc0001ed2a8, 0xc0000ba008)
pod4               | 	/app/internal/server/fasthttp.go:95 +0x45
pod4               | github.com/rbenatti8/rinha-de-backend-2025/internal/server.(*Handler).handleGet(0xc0001ed2a8, 0xc0000ba008)
pod4               | 	/app/internal/server/fasthttp.go:84 +0x85
pod4               | github.com/rbenatti8/rinha-de-backend-2025/internal/server.(*Handler).handler(0xc00057ecb8?, 0x49b0b3?)
pod4               | 	/app/internal/server/fasthttp.go:33 +0x48
pod4               | github.com/valyala/fasthttp.(*Server).serveConn(0xc000144248, {0xa0e6f8, 0xc00004a000})
pod4               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/server.go:2455 +0x11cf
pod4               | github.com/valyala/fasthttp.(*workerPool).workerFunc(0xc000142090, 0xc0002cf7a0)
pod4               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/workerpool.go:225 +0x92
pod4               | github.com/valyala/fasthttp.(*workerPool).getCh.func1()
pod4               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/workerpool.go:197 +0x32
pod4               | created by github.com/valyala/fasthttp.(*workerPool).getCh in goroutine 788
pod4               | 	/go/pkg/mod/github.com/valyala/fasthttp@v1.64.0/workerpool.go:196 +0x194
haproxy-1          | [WARNING]  (8) : Server http_back/web2 is DOWN, reason: Layer4 timeout, check duration: 2001ms. 2 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.
[Kpod4 exited with code 2
haproxy-1          | [WARNING]  (8) : Server http_back/web3 is DOWN, reason: Layer4 timeout, check duration: 2001ms. 1 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.
haproxy-1          | [WARNING]  (8) : Server http_back/web4 is DOWN, reason: Layer4 timeout, check duration: 2003ms. 0 active and 0 backup servers left. 1 sessions active, 0 requeued, 0 remaining in queue.
haproxy-1          | [ALERT]    (8) : backend 'http_back' has no server available!
haproxy-1          | [NOTICE]   (1) : haproxy version is 3.2.3-1844da7
haproxy-1          | [NOTICE]   (1) : path to executable is /usr/local/sbin/haproxy
haproxy-1          | [WARNING]  (1) : Exiting Master process...
haproxy-1          | [WARNING]  (8) : Proxy http_front stopped (cumulated conns: FE: 13, BE: 0).
haproxy-1          | [WARNING]  (8) : Proxy http_back stopped (cumulated conns: FE: 0, BE: 13).
haproxy-1          | [WARNING]  (1) : Current worker (8) exited with code 0 (Exit)
haproxy-1          | [WARNING]  (1) : All workers exited. Exiting... (0)
[Khaproxy-1 exited with code 0
redis-cache-rinha  | 1:signal-handler (1753848249) Received SIGTERM scheduling shutdown...
redis-cache-rinha  | 1:M 30 Jul 2025 04:04:09.752 * User requested shutdown...
redis-cache-rinha  | 1:M 30 Jul 2025 04:04:09.752 * Saving the final RDB snapshot before exiting.
redis-cache-rinha  | 1:M 30 Jul 2025 04:04:09.766 * DB saved on disk
redis-cache-rinha  | 1:M 30 Jul 2025 04:04:09.766 # Redis is now ready to exit, bye bye...
[Kredis-cache-rinha exited with code 0
